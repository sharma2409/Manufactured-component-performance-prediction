---
title: "Regression Models-iiC_Caret"
author: "Raghav Sharma"
date: "4/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(caret)
library(dplyr)
library(tidymodels)
```
```{r}
data_url <- 'https://raw.githubusercontent.com/jyurko/INFSCI_2595_Spring_2021/main/HW/final_project/infsci_2595_s21_final_project_data.csv'

df <- readr::read_csv(data_url, col_names = TRUE)
```
The data will be filtered for only the Step 1 variables

```{r}
df_reg <- df %>% select(-outcome_2,-(x07:x11))
```

The data-set wil now be split into the training set and test set.

```{r, data partition}
set.seed(123)

df_index <- createDataPartition(df_reg$response_1, p = 0.7, list = FALSE)  #70-30 split

df_train <- df_reg[df_index, ]
df_test <- df_reg[-df_index, ]

nrow(df_train)
nrow(df_test)
```

It can be seen that the split was successful

The resampling will be done on the training set.

The resampling scheme used is repeated k-cross validation with number = 10 and repeats = 2

```{r, resampling strategy}
cv <- trainControl(
  method = "repeatedcv",
  
  number = 10, 
  repeats = 2
)

```

The data will be tuned and trained using the following models:

Linear Model:

```{r}
set.seed(123)

(linear_model <- train(
  
  form = response_1 ~ xA + xB + x01 + x02 + x03 + x04 + x05 + x06,
  
  data = df_train,
  
  method = "lm",
  
  trControl = cv,
  
  preProcess = c("center", "scale"),
  
  metric = "RMSE",
  
  tuneGrid = expand.grid(intercept = c(TRUE, FALSE))

))

linear_model
```

GLMNET:

```{r}
elastic_grid <- expand.grid(alpha = seq(0, 1, by = 0.1),
                            lambda = seq(0, 1, by = 0.1) )
```


Subcase 1: Pair-wise interactions
```{r}
set.seed(123)

(elastic_model <- train(
  
  form = response_1 ~ (xA + xB + x01 + x02 + x03 + x04 + x05 + x06)^2,
  
  data = df_train,
  
  method = "glmnet",
  
  trControl = cv,
  
  preProcess = c("center", "scale"),
  
  metric = "RMSE", 
  
  tuneGrid = elastic_grid
))

elastic_model

```
```{r}
coef(elastic_model$finalModel, elastic_model$bestTune$lambda)
```
The number of coefficients for glmnet with pair-wise interactions are 53

Subcase 2: Three way interaction for glmnet

```{r}
set.seed(123)

(elastic_model_2 <- train(
  
  form = response_1 ~ (xA + xB + x01 + x02 + x03 + x04 + x05 + x06)^3,
  
  data = df_train,
  
  method = "glmnet",
  
  trControl = cv,
  
  preProcess = c("center", "scale"),
  
  metric = "RMSE", 
  
  tuneGrid = elastic_grid
))

elastic_model_2

```

```{r}
coef(elastic_model_2$finalModel, elastic_model_2$bestTune$lambda)
```
The number of coefficients for glmnet with three-way interactions are 151

Neural Nets:

```{r}
nnet_grid <- expand.grid(size = c(3, 5, 10, 15),
                         decay = exp(seq(-6, 3, length.out = 31)))
```


```{r}
set.seed(123)

(nnet_model <- train(
  
  form = response_1 ~ xA + xB + x01 + x02 + x03 + x04 + x05 + x06,
  
  data = df_train,
  
  method = "nnet",
  
  trControl = cv,
  
  preProcess = c("center", "scale"),
  
  metric = "RMSE",
  
  tuneGrid = nnet_grid
))

nnet_model
```

Random Forests:

```{r}
set.seed(123)

(rf_model <- train(
  
  form = response_1 ~ (xA + xB + x01 + x02 + x03 + x04 + x05 + x06),
  
  data = df_train,
  
  method = "rf",
  
  trControl = cv,
  
  preProcess = c("center", "scale"),
  
  metric = "RMSE", 
  
  tuneGrid = expand.grid(mtry = seq(2, 8, by = 1)),
  importance = TRUE
))

rf_model
```


XgBoost:


```{r}
set.seed(123)

(xgboost_model_0 <- train(
  
  form = response_1 ~ (xA + xB + x01 + x02 + x03 + x04 + x05 + x06),
  
  data = df_train,
  
  method = "xgbTree",
  
  trControl = cv,
  
  preProcess = c("center", "scale"),
  
  metric = "RMSE",
  
  tuneGrid = expand.grid(nrounds = seq(75, 125, 25), 
                                       max_depth = seq(3,5,1), 
                                       colsample_bytree = c(0.8, 0.9),
                                       eta = c(0.4,0.5), 
                                       gamma = 0,
                                       min_child_weight = 1,
                                       subsample = 0.75)
))
```

The two models chosen are k-nearest neighbor and Partial Least Squares (PLS)

K- nearest neighbor:

```{r}
kknn_grid <- expand.grid(k = seq(2, 25, by = 1))
```

```{r}
set.seed(123)

(kknn_model <- train(
  
  form = response_1 ~ (xA + xB + x01 + x02 + x03 + x04 + x05 + x06),
  
  data = df_train,
  
  method = "knn",
  
  trControl = cv,
  
  preProcess = c("center", "scale"),
  
  metric = "RMSE", 
  
  tuneGrid = kknn_grid
))

kknn_model

```


Partial Least Squares:

```{r}
set.seed(123)
(pls_model <- train(
  
  form = response_1 ~ (xA + xB + x01 + x02 + x03 + x04 + x05 + x06),
  
  data = df_train,
  
  method = "pls",
  
  trControl = cv,
  
  preProcess = c("center", "scale"),
  
  metric = "RMSE", 
  
  tuneLength = 10
  
))

pls_model

```

```{r}
my_results <- resamples(list(Linear = linear_model,
                             Elastic_pair_wise = elastic_model,
                             Elastic_triple_interaction = elastic_model_2,
                             Nnet = nnet_model,
                             Random_forest = rf_model,
                             Xgboost = xgboost_model_0,
                             Knn = kknn_model,
                             PLS = pls_model
                             ))

dotplot(my_results, metric = "RMSE")
```
The best model from all the selected model are random forests.
